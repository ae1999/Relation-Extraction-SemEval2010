{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "promotional-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import tqdm\n",
    "import time\n",
    "import spacy \n",
    "import random\n",
    "import scipy.io\n",
    "import itertools\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from tqdm.contrib import tzip\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "affected-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "occasional-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_repl = re.compile('(<e1>)|(</e1>)|(<e2>)|(</e2>)|(\\'s)')\n",
    "pattern_e1 = re.compile('<e1>(.*)</e1>')\n",
    "pattern_e2 = re.compile('<e2>(.*)</e2>')\n",
    "pattern_symbol = re.compile('^[!\"#$%&\\\\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]|[!\"#$%&\\\\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "quality-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../datasets/part2/TRAIN_FILE.TXT'\n",
    "f = open(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "varied-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_ukn = [float(i) for i in '0.22418134 -0.28881392 0.13854356 0.00365387 -0.12870757 0.10243822 0.061626635 0.07318011 -0.061350107 -1.3477012 0.42037755 -0.063593924 -0.09683349 0.18086134 0.23704372 0.014126852 0.170096 -1.1491593 0.31497982 0.06622181 0.024687296 0.076693475 0.13851812 0.021302193 -0.06640582 -0.010336159 0.13523154 -0.042144544 -0.11938788 0.006948221 0.13333307 -0.18276379 0.052385733 0.008943111 -0.23957317 0.08500333 -0.006894406 0.0015864656 0.063391194 0.19177166 -0.13113557 -0.11295479 -0.14276934 0.03413971 -0.034278486 -0.051366422 0.18891625 -0.16673574 -0.057783455 0.036823478 0.08078679 0.022949161 0.033298038 0.011784158 0.05643189 -0.042776518 0.011959623 0.011552498 -0.0007971594 0.11300405 -0.031369694 -0.0061559738 -0.009043574 -0.415336 -0.18870236 0.13708843 0.005911723 -0.113035575 -0.030096142 -0.23908928 -0.05354085 -0.044904727 -0.20228513 0.0065645403 -0.09578946 -0.07391877 -0.06487607 0.111740574 -0.048649278 -0.16565254 -0.052037314 -0.078968436 0.13684988 0.0757494 -0.006275573 0.28693774 0.52017444 -0.0877165 -0.33010918 -0.1359622 0.114895485 -0.09744406 0.06269521 0.12118575 -0.08026362 0.35256687 -0.060017522 -0.04889904 -0.06828978 0.088740796 0.003964443 -0.0766291 0.1263925 0.07809314 -0.023164088 -0.5680669 -0.037892066 -0.1350967 -0.11351585 -0.111434504 -0.0905027 0.25174105 -0.14841858 0.034635577 -0.07334565 0.06320108 -0.038343467 -0.05413284 0.042197507 -0.090380974 -0.070528865 -0.009174437 0.009069661 0.1405178 0.02958134 -0.036431845 -0.08625681 0.042951006 0.08230793 0.0903314 -0.12279937 -0.013899368 0.048119213 0.08678239 -0.14450377 -0.04424887 0.018319942 0.015026873 -0.100526 0.06021201 0.74059093 -0.0016333034 -0.24960588 -0.023739101 0.016396184 0.11928964 0.13950661 -0.031624354 -0.01645025 0.14079992 -0.0002824564 -0.08052984 -0.0021310581 -0.025350995 0.086938225 0.14308536 0.17146006 -0.13943303 0.048792403 0.09274929 -0.053167373 0.031103406 0.012354865 0.21057427 0.32618305 0.18015954 -0.15881181 0.15322933 -0.22558987 -0.04200665 0.0084689725 0.038156632 0.15188617 0.13274793 0.113756925 -0.095273495 -0.049490947 -0.10265804 -0.27064866 -0.034567792 -0.018810693 -0.0010360252 0.10340131 0.13883452 0.21131058 -0.01981019 0.1833468 -0.10751636 -0.03128868 0.02518242 0.23232952 0.042052146 0.11731903 -0.15506615 0.0063580726 -0.15429358 0.1511722 0.12745973 0.2576985 -0.25486213 -0.0709463 0.17983761 0.054027 -0.09884228 -0.24595179 -0.093028545 -0.028203879 0.094398156 0.09233813 0.029291354 0.13110267 0.15682974 -0.016919162 0.23927948 -0.1343307 -0.22422817 0.14634751 -0.064993896 0.4703685 -0.027190214 0.06224946 -0.091360025 0.21490277 -0.19562101 -0.10032754 -0.09056772 -0.06203493 -0.18876675 -0.10963594 -0.27734384 0.12616494 -0.02217992 -0.16058226 -0.080475815 0.026953284 0.110732645 0.014894041 0.09416802 0.14299914 -0.1594008 -0.066080004 -0.007995227 -0.11668856 -0.13081996 -0.09237365 0.14741232 0.09180138 0.081735 0.3211204 -0.0036552632 -0.047030564 -0.02311798 0.048961394 0.08669574 -0.06766279 -0.50028914 -0.048515294 0.14144728 -0.032994404 -0.11954345 -0.14929578 -0.2388355 -0.019883996 -0.15917352 -0.052084364 0.2801028 -0.0029121689 -0.054581646 -0.47385484 0.17112483 -0.12066923 -0.042173345 0.1395337 0.26115036 0.012869649 0.009291686 -0.0026459037 -0.075331464 0.017840583 -0.26869613 -0.21820338 -0.17084768 -0.1022808 -0.055290595 0.13513643 0.12362477 -0.10980586 0.13980341 -0.20233242 0.08813751 0.3849736 -0.10653763 -0.06199595 0.028849555 0.03230154 0.023856193 0.069950655 0.19310954 -0.077677034 -0.144811'.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "single-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils():\n",
    "    def __init__(self, data_path = data_path, min_word_frequency = 2):\n",
    "        self.min_word_frequency = min_word_frequency\n",
    "        self.english_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "        self.max_cap_length = 30\n",
    "        \n",
    "        print(\"loading data ...\")\n",
    "        self.data = self.load_dataset(data_path)\n",
    "        print(\"---done!---\")\n",
    "        \n",
    "        print(\"creating vocabulary ...\")\n",
    "        self.create_vocabulary(self.data)\n",
    "        \n",
    "        print(\"numericalizing sentences ...\")\n",
    "        self.numericalize_sentences = self.sentences_numericalizer(self.data)\n",
    "        \n",
    "        self.indexes = []\n",
    "        asdf = 0\n",
    "        for i in self.data:\n",
    "            try:\n",
    "                a, b = i[0][0].split()[0], i[0][1].split()[0]\n",
    "                self.indexes.append((i[0][2].split().index(a), i[0][2].split().index(b)))\n",
    "            except:\n",
    "                a, b = i[0][0].split()[0], i[0][1].split()[0]\n",
    "                if not a in i[0][2].split():\n",
    "                    l = 0\n",
    "                else:\n",
    "                    l = i[0][2].split().index(a)\n",
    "                if not b in i[0][2].split():\n",
    "                    k = len(i[0][2].split())-1\n",
    "                else:\n",
    "                    k = i[0][2].split().index(b)\n",
    "                self.indexes.append((l,k))\n",
    "        \n",
    "        self.all_classes = list(np.unique([i[1].split('(')[0] for i in self.data]))\n",
    "        all_classes = self.all_classes\n",
    "        self.classes = []\n",
    "        for i in self.data:\n",
    "            self.classes.append(all_classes.index(i[1].split('(')[0]))\n",
    "            \n",
    "        print(\"sentences:\", len(self.data), \"sentences:\",  len(self.numericalize_sentences), \"Vocab:\", len(self.vocabulary))\n",
    "        \n",
    "        \n",
    "    def delete_symbol(self, text):\n",
    "        if pattern_symbol.search(text):\n",
    "            return pattern_symbol.sub('', text)\n",
    "        return text\n",
    "\n",
    "    def load_dataset(self, path_dataset):\n",
    "        dataset = []\n",
    "        with open(path_dataset) as f:\n",
    "            piece = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    piece.append(line)\n",
    "                elif piece:\n",
    "                    if int(piece[0].split('\\t')[0]) > 7109:\n",
    "                        break\n",
    "                    sentence = piece[0].split('\\t')[1].strip('\"')\n",
    "                    e1 = self.delete_symbol(pattern_e1.findall(sentence)[0])\n",
    "                    e2 = self.delete_symbol(pattern_e2.findall(sentence)[0])\n",
    "                    new_sentence = list()\n",
    "                    for word in pattern_repl.sub('', sentence).split(' '):\n",
    "                        new_word = self.delete_symbol(word)\n",
    "                        if new_word:\n",
    "                            new_sentence.append(new_word)\n",
    "\n",
    "                    relation = piece[1]\n",
    "                    dataset.append(((e1, e2, ' '.join(new_sentence)), relation))\n",
    "                    piece = list()\n",
    "        return dataset\n",
    "    \n",
    "    def tokenizer_eng(self, text):\n",
    "        return [tok.text.lower() for tok in self.english_tokenizer.tokenizer(text)]\n",
    "    \n",
    "    def create_vocabulary(self, data):\n",
    "        self.vocabulary = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.rev_vocabulary = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        min_word_frequency = self.min_word_frequency\n",
    "        \n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        counter, count = 1, len(data)\n",
    "        for datum in data:\n",
    "            sentence = datum[0][2]\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == min_word_frequency:\n",
    "                    self.rev_vocabulary[word] = idx\n",
    "                    self.vocabulary[idx] = word\n",
    "                    idx += 1\n",
    "            counter += 1\n",
    "            print(str(round(counter/count*100, 2))+'%', end=\"\\r\")\n",
    "        print('---done!---')\n",
    "        \n",
    "    def numericalize_sentence(self, caption):\n",
    "        numericalized_caption = [self.rev_vocabulary[\"<SOS>\"]]\n",
    "        numericalized_caption += self.numericalize(caption)\n",
    "        numericalized_caption.append(self.rev_vocabulary[\"<EOS>\"])\n",
    "        return numericalized_caption\n",
    "    \n",
    "    def sentences_numericalizer(self, data):\n",
    "        numericalized_sentences = []\n",
    "        \n",
    "        counter, count = 1, len(data)\n",
    "        for datum in data:\n",
    "            sentence = datum[0][2]\n",
    "            \n",
    "            padded_numericalized = self.pad(self.numericalize_sentence(sentence))\n",
    "            numericalized_sentences.append(padded_numericalized)\n",
    "            counter += 1\n",
    "            print(str(round(counter/count*100, 2))+'%', end=\"\\r\")\n",
    "        print(\"---done!---!\")\n",
    "        return numericalized_sentences\n",
    "        \n",
    "    def pad(self, sequence):\n",
    "        max_cap_length = self.max_cap_length\n",
    "        if len(sequence) > max_cap_length:\n",
    "            sequence = sequence[:max_cap_length]\n",
    "            sequence[max_cap_length-1] = self.rev_vocabulary[\"<EOS>\"]\n",
    "        else:\n",
    "            while(len(sequence) < max_cap_length):\n",
    "                sequence.append(self.rev_vocabulary[\"<PAD>\"])\n",
    "        return sequence\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.rev_vocabulary[token] if token in self.rev_vocabulary else self.rev_vocabulary[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "        \n",
    "    def reverse_numericalize(self, encoded):\n",
    "\n",
    "        strings = []\n",
    "            \n",
    "        for token in encoded:\n",
    "            if token in self.vocabulary:\n",
    "                strings.append(self.vocabulary[token])\n",
    "            else:\n",
    "                strings.append(self.vocabulary[3])\n",
    "            if strings[-1] == '<EOS>':\n",
    "                break\n",
    "        \n",
    "        \n",
    "        if '<SOS>' in strings:\n",
    "            strings.remove('<SOS>')\n",
    "        if '<EOS>' in strings:\n",
    "            strings.remove('<EOS>')\n",
    "        while '<PAD>' in strings:\n",
    "            strings.remove('<PAD>')\n",
    "            \n",
    "        sentence = ''\n",
    "        for i in strings:\n",
    "            sentence += i\n",
    "            sentence += ' '\n",
    "            \n",
    "        return sentence[:-1]\n",
    "    \n",
    "    def load_glove(self):\n",
    "        glove_vocabulary = {}\n",
    "        counter, count = 1, len(self.rev_vocabulary.keys())\n",
    "        with open('../../datasets/glove.42B.300d.txt') as f:\n",
    "            for i in f:\n",
    "                spl = i.split()\n",
    "                if spl[0] in self.rev_vocabulary.keys():\n",
    "                    l = [float(j) for j in spl[1:]]\n",
    "                    glove_vocabulary[self.rev_vocabulary[spl[0]]] = l\n",
    "                    counter += 1\n",
    "                    print(str(round(counter/count*100, 2))+'%', end=\"\\r\")\n",
    "                    \n",
    "        glove_vocabulary[self.rev_vocabulary[\"<PAD>\"]] = glove_vocabulary[self.rev_vocabulary[\".\"]]\n",
    "        glove_vocabulary[self.rev_vocabulary[\"<EOS>\"]] = glove_vocabulary[self.rev_vocabulary[\".\"]]\n",
    "        glove_vocabulary[self.rev_vocabulary[\"<UNK>\"]] = glove_ukn\n",
    "        print(\"---done!---\")\n",
    "        return glove_vocabulary\n",
    "    \n",
    "    def get_sentence_class(self, test_size):\n",
    "        self.sentence_train, self.sentence_test, self.c_trin, self.c_test =\\\n",
    "            train_test_split(self.numericalize_sentences, self.classes, test_size = test_size, random_state=42)\n",
    "        return self.sentence_train, self.sentence_test, self.c_trin, self.c_test\n",
    "    \n",
    "    def get_glove_sentence_class(self, test_size):\n",
    "        print(\"loading glove ...\")\n",
    "        glovee = self.load_glove()\n",
    "        self.glove_vecs = []\n",
    "        for i in self.numericalize_sentences:\n",
    "            self. glove_vecs.append([glovee[j] if j in glovee.keys() else glovee[3] for j in i[1:]])\n",
    "        self.sentence_train, self.sentence_test, self.c_trin, self.c_test =\\\n",
    "            train_test_split(self.glove_vecs, self.classes, test_size = test_size, random_state=42)\n",
    "        return self.sentence_train, self.sentence_test, self.c_trin, self.c_test\n",
    "    \n",
    "    def get_glove_indexed_sentence_class(self, test_size):\n",
    "        print(\"loading glove ...\")\n",
    "        glovee = self.load_glove()\n",
    "        self.glove_vecs = []\n",
    "        for i in self.numericalize_sentences:\n",
    "            self. glove_vecs.append([glovee[j] if j in glovee.keys() else glovee[3] for j in i[1:]])\n",
    "        out_data = []\n",
    "        for i,j in zip(self.glove_vecs, self.indexes):\n",
    "            out_data.append([i, j])\n",
    "            \n",
    "        self.sentence_train, self.sentence_test, self.c_trin, self.c_test =\\\n",
    "            train_test_split(out_data, self.classes, test_size = test_size, random_state=42)\n",
    "        return self.sentence_train, self.sentence_test, self.c_trin, self.c_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "preceding-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentences, classes):\n",
    "        self.X, self.y = sentences, classes\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        classs = self.y[index]\n",
    "        sentence = self.X[index]\n",
    "        \n",
    "        return torch.tensor(sentence), classs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial-coalition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "---done!---\n",
      "creating vocabulary ...\n",
      "---done!---\n",
      "numericalizing sentences ...\n",
      "---done!---!\n",
      "sentences: 7109 sentences: 7109 Vocab: 8458\n"
     ]
    }
   ],
   "source": [
    "a = DataUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sporting-blade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cause-Effect',\n",
       " 'Component-Whole',\n",
       " 'Content-Container',\n",
       " 'Entity-Destination',\n",
       " 'Entity-Origin',\n",
       " 'Instrument-Agency',\n",
       " 'Member-Collection',\n",
       " 'Message-Topic',\n",
       " 'Other',\n",
       " 'Product-Producer']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.all_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_x86",
   "language": "python",
   "name": "pytorch_x86"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
